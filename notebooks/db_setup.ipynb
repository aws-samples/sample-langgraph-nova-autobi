{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5631bae8-785c-49a1-a5db-033a7c9893ab",
   "metadata": {},
   "source": [
    "# Create config files and Creating Glue database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15c2b9",
   "metadata": {},
   "source": [
    "## Create a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88576f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/config.py\n",
    "\n",
    "PYTHON_GENERATION_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "SQL_GENERATION_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "AWS_REGION = \"us-west-2\"\n",
    "DATABASE_NAME = \"insurance_and_eclaims\"\n",
    "BUCKET_NAME = \"<Your-bucket-name>\"\n",
    "ATHENA_S3_OUTPUT = f\"s3://{BUCKET_NAME}/athena_out/\"\n",
    "ATHENA_WORKGROUP = \"primary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c8c34-1987-4c67-8c30-c2a847ce6f9f",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee1dd3-544d-47b0-8a1f-b55dd8144946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from config import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fea95",
   "metadata": {},
   "source": [
    "# User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be210fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_dir = os.path.join(\"..\", \"data\", \"spider_data\", \"database\", \"insurance_and_eClaims\")\n",
    "csv_dir = os.path.join(\"..\", \"data\", \"spider_data_csv\")\n",
    "\n",
    "BUCKET_PATH = f's3://{BUCKET_NAME}/spider_data_csv/'\n",
    "DATABASE_NAME = 'insurance_and_eclaims'\n",
    "CRAWLER_NAME = 'insurance_and_eclaims_crawler'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f38d64",
   "metadata": {},
   "source": [
    "# Create Glue Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def create_glue_crawler_role(role_name, bucket_name):\n",
    "    iam = boto3.client('iam')\n",
    "\n",
    "    # Define the trust relationship policy\n",
    "    trust_relationship = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"glue.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the IAM role\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_relationship)\n",
    "        )\n",
    "        print(f\"Role {role_name} created successfully.\")\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f\"Role {role_name} already exists.\")\n",
    "\n",
    "    # Attach the AWS Glue Service Role policy\n",
    "    iam.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'\n",
    "    )\n",
    "    print(\"AWSGlueServiceRole policy attached.\")\n",
    "\n",
    "    # Define the S3 bucket access policy\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket_name}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create and attach the S3 bucket access policy\n",
    "    iam.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName=f\"GlueCrawlerS3Policy-{bucket_name}\",\n",
    "        PolicyDocument=json.dumps(s3_policy)\n",
    "    )\n",
    "    print(f\"S3 bucket access policy for {bucket_name} attached.\")\n",
    "\n",
    "    # Get and return the role ARN\n",
    "    role = iam.get_role(RoleName=role_name)\n",
    "    return role['Role']['Arn']\n",
    "\n",
    "# Usage\n",
    "role_name = \"GlueCrawlerRole-NovaBlog\"\n",
    "\n",
    "IAM_ROLE_ARN = create_glue_crawler_role(role_name, BUCKET_NAME)\n",
    "print(f\"Glue Crawler Role ARN: {IAM_ROLE_ARN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57240dc-d2a2-4bcf-9c37-3543dbab8fc9",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ec104-1cfc-41e0-b62c-7ff29519d671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_name(name):\n",
    "    return ''.join(c if c.isalnum() else '_' for c in name).rstrip('_')\n",
    "\n",
    "def sqlite_to_csv(sqlite_file, csv_base_dir):\n",
    "    db_name = sanitize_name(os.path.splitext(os.path.basename(sqlite_file))[0])\n",
    "    \n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    \n",
    "    # Create a cursor\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get list of tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    for table_name in tables:\n",
    "        table_name = table_name[0]\n",
    "        try:\n",
    "            # Read the table into a pandas DataFrame\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM '{table_name}'\", conn)\n",
    "            \n",
    "            # Add columns for metadata\n",
    "            df['source_database'] = db_name\n",
    "            df['source_table'] = table_name\n",
    "            \n",
    "            # Create the output directory\n",
    "            output_dir = os.path.join(csv_base_dir, db_name, sanitize_name(table_name))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Write the DataFrame to a CSV file\n",
    "            csv_file = os.path.join(output_dir, f\"{sanitize_name(table_name)}.csv\")\n",
    "            df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Successfully converted table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {table_name}: {str(e)}\")\n",
    "            print(f\"Skipping table {table_name} and moving to the next one.\")\n",
    "            continue\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7b027-1bed-4cb2-8261-4a6882788bf9",
   "metadata": {},
   "source": [
    "## Convert to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34528ac3-3b94-4616-b911-f007d956c385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_folders = []\n",
    "not_processed_folders = []\n",
    "problematic_databases = []\n",
    "\n",
    "# Create the base CSV directory if it doesn't exist\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "# Iterate through the folders\n",
    "for root, dirs, files in os.walk(sqlite_dir):\n",
    "    sqlite_files = [file for file in files if file.endswith('.sqlite')]\n",
    "    \n",
    "    if sqlite_files:\n",
    "        processed_folders.append(os.path.relpath(root, sqlite_dir))\n",
    "        \n",
    "        for file in sqlite_files:\n",
    "            sqlite_path = os.path.join(root, file)\n",
    "            \n",
    "            print(f\"\\nConverting {sqlite_path} to CSV...\")\n",
    "            try:\n",
    "                sqlite_to_csv(sqlite_path, csv_dir)\n",
    "                print(f\"Conversion complete for {sqlite_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing database {sqlite_path}: {str(e)}\")\n",
    "                problematic_databases.append(f\"{sqlite_path}: {str(e)}\")\n",
    "                print(f\"Skipping database {sqlite_path} and moving to the next one.\")\n",
    "                continue\n",
    "    elif os.path.isdir(os.path.join(sqlite_dir, root)) and not any(file.endswith('.sqlite') for file in files):\n",
    "        not_processed_folders.append(os.path.relpath(root, sqlite_dir))\n",
    "\n",
    "print(\"All conversions completed. Summary written to 'conversion_summary.txt'.\")\n",
    "print(f\"CSV files are stored in the '{csv_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1101f8-7f6b-4130-9ca4-2e5f175a2247",
   "metadata": {},
   "source": [
    "# S3 sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76121662-4d90-4db1-82de-af5ee307f45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 cp {csv_dir} s3://{BUCKET_NAME}/spider_data_csv/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe69cb-0bf2-4fd4-8c4c-979e67b576a3",
   "metadata": {},
   "source": [
    "# Glue database and Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37927b18-d064-4633-941b-288e1ca847f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "def create_glue_database_and_crawler(bucket_path, database_name, crawler_name, iam_role_arn):\n",
    "    \"\"\"\n",
    "    Creates a Glue crawler to catalog S3 data and create a database\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_path (str): S3 path (e.g., 's3://my-bucket/data/')\n",
    "    database_name (str): Name for the Glue database\n",
    "    crawler_name (str): Name for the Glue crawler\n",
    "    iam_role_arn (str): IAM role ARN with necessary permissions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Glue client\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    try:\n",
    "        # Create Glue Database\n",
    "        print(f\"Creating database: {database_name}\")\n",
    "        glue_client.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_name,\n",
    "                'Description': f'Database created for {bucket_path}'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create Glue Crawler\n",
    "        print(f\"Creating crawler: {crawler_name}\")\n",
    "        glue_client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=iam_role_arn,\n",
    "            DatabaseName=database_name,\n",
    "            Targets={\n",
    "                'S3Targets': [\n",
    "                    {\n",
    "                        'Path': bucket_path\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            Description=f'Crawler for {bucket_path}',\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Start the crawler\n",
    "        print(\"Starting crawler...\")\n",
    "        glue_client.start_crawler(Name=crawler_name)\n",
    "        \n",
    "        # Wait for crawler to complete\n",
    "        while True:\n",
    "            crawler_state = glue_client.get_crawler(Name=crawler_name)['Crawler']['State']\n",
    "            if crawler_state == 'READY':\n",
    "                break\n",
    "            print(f\"Crawler state: {crawler_state}\")\n",
    "            time.sleep(30)\n",
    "        \n",
    "        print(\"Crawler completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85259334-d4d9-44e6-a25a-5f9ccafe7d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_glue_database_and_crawler(\n",
    "    bucket_path=BUCKET_PATH,\n",
    "    database_name=DATABASE_NAME,\n",
    "    crawler_name=CRAWLER_NAME,\n",
    "    iam_role_arn=IAM_ROLE_ARN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
